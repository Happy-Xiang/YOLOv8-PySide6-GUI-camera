from ultralytics.yolo.engine.predictor import BasePredictor
from ultralytics.yolo.engine.results import Results
from ultralytics.yolo.utils import DEFAULT_CFG, LOGGER, SETTINGS, callbacks, ops
from ultralytics.yolo.utils.plotting import Annotator, colors, save_one_box
from ultralytics.yolo.utils.torch_utils import smart_inference_mode
from ultralytics.yolo.utils.files import increment_path
from ultralytics.yolo.utils.checks import check_imshow
from ultralytics.yolo.cfg import get_cfg
from PySide6.QtWidgets import QApplication, QMainWindow, QFileDialog, QMenu
from PySide6.QtGui import QImage, QPixmap, QColor
from PySide6.QtCore import QTimer, QThread, Signal, QObject, QPoint, Qt
from ui.CustomMessageBox import MessageBox
from ui.home import Ui_MainWindow
from UIFunctions import *
from collections import defaultdict
from pathlib import Path
from utils.capnums import Camera
from utils.rtsp_win import Window
from PIL import Image
import torch

import numpy as np
import time
import json
import torch
import sys
import cv2
import os


class YoloPredictor(BasePredictor, QObject):
    yolo2main_pre_img = Signal(np.ndarray)  # raw image signal
    yolo2main_res_img = Signal(np.ndarray)  # test result signal
    yolo2main_status_msg = Signal(str)  # Detecting/pausing/stopping/testing complete/error reporting signal
    yolo2main_fps = Signal(str)  # fps
    yolo2main_labels = Signal(dict)  # Detected target results (number of each category)
    yolo2main_progress = Signal(int)  # Completeness
    yolo2main_class_num = Signal(int)  # Number of categories detected
    yolo2main_target_num = Signal(int)  # Targets detected

    def __init__(self, cfg=DEFAULT_CFG, overrides=None):
        super(YoloPredictor, self).__init__()
        QObject.__init__(self)

        self.args = get_cfg(cfg, overrides)
        project = self.args.project or Path(SETTINGS['runs_dir']) / self.args.task
        name = f'{self.args.mode}'
        self.save_dir = increment_path(Path(project) / name, exist_ok=self.args.exist_ok)
        self.done_warmup = False
        if self.args.show:
            self.args.show = check_imshow(warn=True)

        # æˆ‘å®šä¹‰çš„å±žæ€§
        self.iscamera = 0
        self.capture_frame = None

        # GUI args
        self.used_model_name = None  # The detection model name to use
        self.new_model_name = None  # Models that change in real time
        self.source = ''  # input source
        self.stop_dtc = False  # Termination detection
        self.continue_dtc = True  # pause
        self.save_res = False  # Save test results
        self.save_txt = False  # save label(txt) file
        self.iou_thres = 0.45  # iou
        self.conf_thres = 0.25  # conf
        self.speed_thres = 10  # delay, ms
        self.labels_dict = {}  # return a dictionary of results
        self.progress_value = 0  # progress bar

        # Usable if setup is done
        self.model = None
        self.data = self.args.data  # data_dict
        self.imgsz = None
        self.device = None
        self.dataset = None
        self.vid_path, self.vid_writer = None, None
        self.annotator = None
        self.data_path = None
        self.source_type = None
        self.batch = None
        self.callbacks = defaultdict(list, callbacks.default_callbacks)  # add callbacks
        callbacks.add_integration_callbacks(self)

    ####################################è‡ªå®šä¹‰

    @smart_inference_mode()
    def camera_run(self):

        # æ£€æŸ¥æ˜¯å¦ç”¨GPUåŠ é€Ÿ
        if torch.cuda.is_available():
            print("CUDA is available")
        else:
            print("CUDA is not available")

        try:
            if self.args.verbose:
                LOGGER.info('')

            # set model
            self.yolo2main_status_msg.emit('Loding Model...')
            print('åŠ è½½æ¨¡åž‹')

            if not self.model:
                self.setup_model(self.new_model_name)  # è½½å…¥æ¨¡åž‹
                self.used_model_name = self.new_model_name

            # Check save path/label
            print('æ£€æŸ¥ä¿å­˜è·¯å¾„/æ ‡ç­¾')
            if self.save_res or self.save_txt:
                (self.save_dir / 'labels' if self.save_txt else self.save_dir).mkdir(parents=True, exist_ok=True)

            self.seen, self.windows, self.dt, self.batch = 0, [], (ops.Profile(), ops.Profile(), ops.Profile()), None
            # åˆ›å»ºåä¸º dt çš„å®žä¾‹å˜é‡ï¼Œç”¨äºŽå­˜å‚¨ä¸€ä¸ªå…ƒç»„ï¼Œå¹¶å°†å…¶åˆå§‹åŒ–ä¸ºåŒ…å«ä¸‰ä¸ªå¯¹è±¡ ops.Profile() çš„å…ƒç»„ã€‚
            # ops.Profile() æ˜¯æŒ‡ä»Ž ops æ¨¡å—ä¸­å¯¼å…¥åä¸º Profile() çš„å¯¹è±¡ã€‚

            print('start detection')
            # å¼€å§‹ç‰©ä½“æ£€æµ‹

            count = 0  # run location frame
            start_time = time.time()  # used to calculate the frame rate

            # ------------------
            self.capture = cv2.VideoCapture(0)   # åˆ›å»ºä¸€ä¸ª OpenCV è§†é¢‘æ•èŽ·å¯¹è±¡ï¼Œå‚æ•° 0 è¡¨ç¤ºä½¿ç”¨é»˜è®¤æ‘„åƒå¤´
            self.capture.set(cv2.CAP_PROP_FRAME_WIDTH, 640)  # è®¾ç½®æ‘„åƒå¤´çš„å®½åº¦ä¸º 640
            self.capture.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)  # è®¾ç½®æ‘„åƒå¤´çš„é«˜åº¦ä¸º 480
            self.capture.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc(*"MJPG"))  # è®¾ç½®è§†é¢‘ç¼–ç æ ¼å¼ä¸º MJPG
            self.capture.set(cv2.CAP_PROP_FPS, 200)

            while True:
                # time.sleep(0.06)  # ä¼‘çœ  60 æ¯«ç§’ï¼ˆ0.06 ç§’ï¼‰
                time.sleep(1/200)   # ä¼‘çœ ä¸€æ®µæ—¶é—´ï¼Œè¿™é‡Œè®¾ç½®çš„æ˜¯ 200 FPS
                ret, frame = self.capture.read()  # ä»Žæ‘„åƒå¤´è¯»å–ä¸€å¸§
                if ret:  # å¦‚æžœè¯»å–æˆåŠŸ
                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # å°† BGR å›¾åƒè½¬æ¢ä¸º RGB å›¾åƒ
                    self.capture_frame = Image.fromarray(frame_rgb)  # ä½¿ç”¨ä»Žæ•°ç»„åˆ›å»ºçš„ PIL å›¾åƒ

                # print('è®¾ç½®èµ„æºæ¨¡å¼')
                print(self.capture_frame)
                self.setup_source(self.capture_frame)

                # warmup model
                # çƒ­èº«æ¨¡åž‹
                if not self.done_warmup:
                    # è°ƒç”¨æ¨¡åž‹çš„ warmup å‡½æ•°ï¼Œå…¶ä¸­ imgsz å‚æ•°ä¸ºè¾“å…¥å›¾åƒçš„å¤§å°
                    # å¦‚æžœæ¨¡åž‹ä½¿ç”¨ PyTorchï¼Œimgsz å‚æ•°åº”ä¸º [batch_size, channels, height, width]
                    # å¦‚æžœæ¨¡åž‹ä½¿ç”¨ Tritonï¼Œimgsz å‚æ•°åº”ä¸º [height, width, channels, batch_size]
                    self.model.warmup(
                        imgsz=(1 if self.model.pt or self.model.triton else self.dataset.bs, 3, *self.imgsz))
                    # å°† done_warmup æ ‡è®°ä¸º Trueï¼Œä»¥æ ‡è®°æ¨¡åž‹å·²ç»çƒ­èº«è¿‡
                    self.done_warmup = True
                    print('çƒ­èº«å®Œæ¯•')

                batch = iter(self.dataset)

                # pause switch  ç”¨äºŽæŽ§åˆ¶ç¨‹åºçš„æš‚åœå’Œç»§ç»­
                if self.continue_dtc:
                    # time.sleep(0.001)
                    self.yolo2main_status_msg.emit('Detecting...')
                    batch = next(self.dataset)  # next data

                    self.batch = batch
                    path, im, im0s, vid_cap, s = batch
                    visualize = increment_path(self.save_dir / Path(path).stem,
                                               mkdir=True) if self.args.visualize else False

                    # Calculation completion and frame rate (to be optimized)
                    count += 1  # frame count +1
                    all_count = 1000  # all_count å¯ä»¥è°ƒæ•´ï¼ï¼ï¼
                    self.progress_value = int(count / all_count * 1000)  # progress bar(0~1000)
                    if count % 5 == 0 and count >= 5:  # Calculate the frame rate every 5 frames
                        self.yolo2main_fps.emit(str(int(5 / (time.time() - start_time))))
                        start_time = time.time()

                    # preprocess
                    # self.dt åŒ…å«äº†ä¸‰ä¸ª DetectorTime ç±»åž‹çš„å¯¹è±¡ï¼Œè¡¨ç¤ºé¢„å¤„ç†ã€æŽ¨ç†å’ŒåŽå¤„ç†æ‰€èŠ±è´¹çš„æ—¶é—´

                    ## ä½¿ç”¨ with è¯­å¥è®°å½•ä¸‹ä¸‹ä¸€è¡Œä»£ç æ‰€èŠ±è´¹çš„æ—¶é—´ï¼Œself.dt[0] è¡¨ç¤ºè®°å½•é¢„å¤„ç†æ“ä½œæ‰€èŠ±è´¹çš„æ—¶é—´ã€‚
                    print('preprocess...')
                    with self.dt[0]:
                        # è°ƒç”¨ self.preprocess æ–¹æ³•å¯¹å›¾åƒè¿›è¡Œå¤„ç†ï¼Œå¹¶å°†å¤„ç†åŽçš„å›¾åƒèµ‹å€¼ç»™ im å˜é‡ã€‚
                        im = self.preprocess(im)
                        # å¦‚æžœ im çš„ç»´åº¦ä¸º 3ï¼ˆRGB å›¾åƒï¼‰ï¼Œåˆ™è¡¨ç¤ºè¿™æ˜¯ä¸€å¼ å•å¼ å›¾åƒï¼Œéœ€è¦å°†å…¶æ‰©å±•æˆ 4 ç»´ï¼ŒåŠ ä¸Š batch ç»´åº¦ã€‚
                        if len(im.shape) == 3:
                            im = im[None]  # expand for batch dim  æ‰©å¤§æ‰¹é‡è°ƒæš—
                    # inference
                    with self.dt[1]:
                        # è°ƒç”¨æ¨¡åž‹å¯¹å›¾åƒè¿›è¡ŒæŽ¨ç†ï¼Œå¹¶å°†ç»“æžœèµ‹å€¼ç»™ preds å˜é‡ã€‚
                        preds = self.model(im, augment=self.args.augment, visualize=visualize)
                    # postprocess
                    with self.dt[2]:
                        # è°ƒç”¨ self.postprocess æ–¹æ³•å¯¹æŽ¨ç†ç»“æžœè¿›è¡ŒåŽå¤„ç†ï¼Œå¹¶å°†ç»“æžœä¿å­˜åˆ° self.results å˜é‡ä¸­ã€‚
                        # å…¶ä¸­ preds æ˜¯æ¨¡åž‹çš„é¢„æµ‹ç»“æžœï¼Œim æ˜¯æ¨¡åž‹è¾“å…¥çš„å›¾åƒï¼Œè€Œ im0s æ˜¯åŽŸå§‹å›¾åƒçš„å¤§å°ã€‚
                        self.results = self.postprocess(preds, im, im0s)

                    # visualize, save, write results
                    print('visualize, save, write results...')
                    n = len(im)  # To be improved: support multiple img


                    for i in range(n):
                        self.results[i].speed = {
                            'preprocess': self.dt[0].dt * 1E3 / n,
                            'inference': self.dt[1].dt * 1E3 / n,
                            'postprocess': self.dt[2].dt * 1E3 / n
                        }
                        p, im0 = (path[i], im0s[i].copy()) if self.source_type.webcam or self.source_type.from_img \
                            else (path, im0s.copy())

                        p = Path(p)  # the source dir

                        # s:::   video 1/1 (6/6557) 'path':
                        # must, to get boxs\labels
                        label_str = self.write_results(i, self.results, (p, im, im0))  # labels   /// original :s +=

                        # labels and nums dict
                        class_nums = 0
                        target_nums = 0
                        self.labels_dict = {}
                        if 'no detections' in label_str:
                            pass
                        else:
                            for ii in label_str.split(',')[:-1]:

                                nums, label_name = ii.split('~')

                                li = nums.split(':')[-1]

                                print(li)

                                self.labels_dict[label_name] = int(li)
                                target_nums += int(li)
                                class_nums += 1

                        # save img or video result
                        if self.save_res:
                            self.save_preds(vid_cap, i, str(self.save_dir / p.name))

                        # Send test results ã€å‘é€ä¿¡å· ç»™ label æ˜¾ç¤ºå›¾åƒã€‘
                        self.yolo2main_res_img.emit(im0)  # after detection  ----------ç»“æžœ
                        self.yolo2main_pre_img.emit(im0s if isinstance(im0s, np.ndarray) else im0s[0])  # Before testing
                        # self.yolo2main_labels.emit(self.labels_dict)        # webcam need to change the def write_results
                        self.yolo2main_class_num.emit(class_nums)
                        self.yolo2main_target_num.emit(target_nums)

                        print('send success!')

                        if self.speed_thres != 0:
                            time.sleep(self.speed_thres / 1000)  # delay , ms

                    self.yolo2main_progress.emit(self.progress_value)  # progress bar

                # Detection completed
                if count + 1 >= all_count:
                    if isinstance(self.vid_writer[-1], cv2.VideoWriter):
                        self.vid_writer[-1].release()  # release final video writer
                    self.yolo2main_status_msg.emit('Detection completed')
                    break

        except Exception as e:

            self.capture.release()  # é‡Šæ”¾è§†é¢‘è®¾å¤‡
            print('error:', e)
            self.yolo2main_status_msg.emit('%s' % e)

    ##################################è‡ªå®šä¹‰ ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼

    # main for detect
    @smart_inference_mode()
    def run(self):
        try:

            if self.args.verbose:
                LOGGER.info('')

            # set model
            self.yolo2main_status_msg.emit('Loding Model...')
            print('åŠ è½½æ¨¡åž‹')

            if not self.model:
                self.setup_model(self.new_model_name)
                self.used_model_name = self.new_model_name

            # set source  [è§†é¢‘èµ„æº]
            print('è®¾ç½®èµ„æºæ¨¡å¼')
            self.setup_source(self.source if self.source is not None else self.args.source)

            # Check save path/label
            print('æ£€æŸ¥ä¿å­˜è·¯å¾„/æ ‡ç­¾')
            if self.save_res or self.save_txt:
                (self.save_dir / 'labels' if self.save_txt else self.save_dir).mkdir(parents=True, exist_ok=True)

            # warmup model
            # çƒ­èº«æ¨¡åž‹
            if not self.done_warmup:
                # è°ƒç”¨æ¨¡åž‹çš„ warmup å‡½æ•°ï¼Œå…¶ä¸­ imgsz å‚æ•°ä¸ºè¾“å…¥å›¾åƒçš„å¤§å°
                # å¦‚æžœæ¨¡åž‹ä½¿ç”¨ PyTorchï¼Œimgsz å‚æ•°åº”ä¸º [batch_size, channels, height, width]
                # å¦‚æžœæ¨¡åž‹ä½¿ç”¨ Tritonï¼Œimgsz å‚æ•°åº”ä¸º [height, width, channels, batch_size]
                self.model.warmup(imgsz=(1 if self.model.pt or self.model.triton else self.dataset.bs, 3, *self.imgsz))
                # å°† done_warmup æ ‡è®°ä¸º Trueï¼Œä»¥æ ‡è®°æ¨¡åž‹å·²ç»çƒ­èº«è¿‡
                self.done_warmup = True
            print('çƒ­èº«å®Œæ¯•')

            self.seen, self.windows, self.dt, self.batch = 0, [], (ops.Profile(), ops.Profile(), ops.Profile()), None
            # åˆ›å»ºåä¸º dt çš„å®žä¾‹å˜é‡ï¼Œç”¨äºŽå­˜å‚¨ä¸€ä¸ªå…ƒç»„ï¼Œå¹¶å°†å…¶åˆå§‹åŒ–ä¸ºåŒ…å«ä¸‰ä¸ªå¯¹è±¡ ops.Profile() çš„å…ƒç»„ã€‚
            # ops.Profile() æ˜¯æŒ‡ä»Ž ops æ¨¡å—ä¸­å¯¼å…¥åä¸º Profile() çš„å¯¹è±¡ã€‚

            print('start detection')
            # start detection
            # for batch in self.dataset:

            count = 0  # run location frame
            start_time = time.time()  # used to calculate the frame rate
            batch = iter(self.dataset)
            while True:
                # Termination detection  ã€ç»ˆæ­¢æ£€æµ‹ã€‘
                if self.stop_dtc:
                    # é‡Šæ”¾CV2è§†é¢‘å†™å…¥å™¨
                    if isinstance(self.vid_writer[-1], cv2.VideoWriter):
                        self.vid_writer[-1].release()  # release final video writer
                    self.yolo2main_status_msg.emit('Detection terminated!')
                    break

                # Change the model midway ã€åˆ‡æ¢modelã€‘  å¦‚æžœä¸ç›¸ç­‰ï¼Œåˆ™æ‰§è¡Œ setup_model() æ–¹æ³•è®¾ç½®æ–°çš„æ¨¡åž‹
                if self.used_model_name != self.new_model_name:
                    # self.yolo2main_status_msg.emit('Change Model...')
                    self.setup_model(self.new_model_name)
                    self.used_model_name = self.new_model_name

                # pause switch  ç”¨äºŽæŽ§åˆ¶ç¨‹åºçš„æš‚åœå’Œç»§ç»­
                if self.continue_dtc:
                    # time.sleep(0.001)
                    self.yolo2main_status_msg.emit('Detecting...')
                    batch = next(self.dataset)  # next data

                    self.batch = batch
                    path, im, im0s, vid_cap, s = batch
                    visualize = increment_path(self.save_dir / Path(path).stem,
                                               mkdir=True) if self.args.visualize else False

                    # Calculation completion and frame rate (to be optimized)
                    count += 1  # frame count +1
                    if vid_cap:
                        all_count = vid_cap.get(cv2.CAP_PROP_FRAME_COUNT)  # total frames
                    else:
                        all_count = 1
                    self.progress_value = int(count / all_count * 1000)  # progress bar(0~1000)
                    if count % 5 == 0 and count >= 5:  # Calculate the frame rate every 5 frames
                        self.yolo2main_fps.emit(str(int(5 / (time.time() - start_time))))
                        start_time = time.time()

                    # preprocess
                    # self.dt åŒ…å«äº†ä¸‰ä¸ª DetectorTime ç±»åž‹çš„å¯¹è±¡ï¼Œè¡¨ç¤ºé¢„å¤„ç†ã€æŽ¨ç†å’ŒåŽå¤„ç†æ‰€èŠ±è´¹çš„æ—¶é—´

                    ## ä½¿ç”¨ with è¯­å¥è®°å½•ä¸‹ä¸‹ä¸€è¡Œä»£ç æ‰€èŠ±è´¹çš„æ—¶é—´ï¼Œself.dt[0] è¡¨ç¤ºè®°å½•é¢„å¤„ç†æ“ä½œæ‰€èŠ±è´¹çš„æ—¶é—´ã€‚
                    with self.dt[0]:
                        # è°ƒç”¨ self.preprocess æ–¹æ³•å¯¹å›¾åƒè¿›è¡Œå¤„ç†ï¼Œå¹¶å°†å¤„ç†åŽçš„å›¾åƒèµ‹å€¼ç»™ im å˜é‡ã€‚
                        im = self.preprocess(im)
                        # å¦‚æžœ im çš„ç»´åº¦ä¸º 3ï¼ˆRGB å›¾åƒï¼‰ï¼Œåˆ™è¡¨ç¤ºè¿™æ˜¯ä¸€å¼ å•å¼ å›¾åƒï¼Œéœ€è¦å°†å…¶æ‰©å±•æˆ 4 ç»´ï¼ŒåŠ ä¸Š batch ç»´åº¦ã€‚
                        if len(im.shape) == 3:
                            im = im[None]  # expand for batch dim  æ‰©å¤§æ‰¹é‡è°ƒæš—
                    # inference
                    with self.dt[1]:
                        # è°ƒç”¨æ¨¡åž‹å¯¹å›¾åƒè¿›è¡ŒæŽ¨ç†ï¼Œå¹¶å°†ç»“æžœèµ‹å€¼ç»™ preds å˜é‡ã€‚
                        preds = self.model(im, augment=self.args.augment, visualize=visualize)
                    # postprocess
                    with self.dt[2]:
                        # è°ƒç”¨ self.postprocess æ–¹æ³•å¯¹æŽ¨ç†ç»“æžœè¿›è¡ŒåŽå¤„ç†ï¼Œå¹¶å°†ç»“æžœä¿å­˜åˆ° self.results å˜é‡ä¸­ã€‚
                        # å…¶ä¸­ preds æ˜¯æ¨¡åž‹çš„é¢„æµ‹ç»“æžœï¼Œim æ˜¯æ¨¡åž‹è¾“å…¥çš„å›¾åƒï¼Œè€Œ im0s æ˜¯åŽŸå§‹å›¾åƒçš„å¤§å°ã€‚
                        self.results = self.postprocess(preds, im, im0s)



                    # visualize, save, write results
                    n = len(im)  # To be improved: support multiple img
                    for i in range(n):
                        self.results[i].speed = {
                            'preprocess': self.dt[0].dt * 1E3 / n,
                            'inference': self.dt[1].dt * 1E3 / n,
                            'postprocess': self.dt[2].dt * 1E3 / n}
                        p, im0 = (path[i], im0s[i].copy()) if self.source_type.webcam or self.source_type.from_img \
                            else (path, im0s.copy())

                        p = Path(p)  # the source dir

                        # s:::   video 1/1 (6/6557) 'path':
                        # must, to get boxs\labels
                        label_str = self.write_results(i, self.results, (p, im, im0))  # labels   /// original :s +=

                        # labels and nums dict
                        class_nums = 0
                        target_nums = 0
                        self.labels_dict = {}
                        if 'no detections' in label_str:
                            pass
                        else:
                            for ii in label_str.split(',')[:-1]:
                                nums, label_name = ii.split('~')
                                self.labels_dict[label_name] = int(nums)
                                target_nums += int(nums)
                                class_nums += 1

                        # save img or video result
                        if self.save_res:
                            self.save_preds(vid_cap, i, str(self.save_dir / p.name))

                        # Send test results ã€å‘é€ä¿¡å· ç»™ label æ˜¾ç¤ºå›¾åƒã€‘

                        #  emit() æ–¹æ³•ï¼Œè¿™ç§æ–¹æ³•å¸¸å¸¸ç”¨äºŽPySide/PyQtä¿¡å·æœºåˆ¶.
                        #  emit() æ–¹æ³•å®žçŽ°äº†ä¿¡å·æœºåˆ¶çš„å‘é€æ“ä½œï¼Œä¹Ÿå°±æ˜¯å‘å…³è”çš„æ§½å‡½æ•°å‘é€ä¿¡å·å€¼.
                        # å½“æˆ‘ä»¬åœ¨ä¸Šè¿° emit() æ–¹æ³•ä¸­ä¼ å…¥äº†ä¸€ä¸ªå‚æ•°ï¼Œè¿™ä¸ªå‚æ•°å°±ä¼šè¢«å‘é€ç»™ä¸€ä¸ªæŽ¥æ”¶æ§½å‡½æ•°å¹¶å¤„ç†å®ƒã€‚
                        # emit() æ˜¯ä¸€ä¸ªåŒæ­¥æ“ä½œ, å®ƒåœ¨ä¿¡å·å‘å‡ºåŽä¼šç›´æŽ¥è¿›å…¥åˆ°ä¸Žä¹‹è¿žæŽ¥çš„æ§½å‡½æ•°
                        # è€Œä¸æ˜¯åƒå¤šçº¿ç¨‹ä¸€æ ·ç­‰å¾…è¢«æ‰§è¡Œã€‚æ‰€ä»¥ å¯ä»¥è¯´emitä¸æ˜¯å¼‚æ­¥çš„ã€‚

                        # åœ¨ PySide6 æˆ– PyQt6 ä¸­ï¼Œemit() å‘å‡ºçš„ä¿¡å·è°ƒç”¨å°†ä¼šå¼‚æ­¥åœ°å°†ä¿¡å·æ”¾å…¥äº‹ä»¶é˜Ÿåˆ—é‡Œï¼Œ
                        # ä¹‹åŽåœ¨äº‹ä»¶å¾ªçŽ¯ä¸­è¿›è¡Œå¤„ç†ï¼Œå¦‚æžœè¿™ä¸ªä¿¡å·ä¸Žå¤šä¸ªæ§½å‡½æ•°è¿žæŽ¥ï¼Œ
                        # é‚£ä¹ˆè¿™äº›æ§½å‡½æ•°å°†ä¼šæŒ‰å…ˆåŽé¡ºåºè¢«å¼‚æ­¥åœ°è°ƒç”¨ã€‚
                        # ä¹Ÿå°±æ˜¯è¯´ï¼Œè™½ç„¶ emit() æ“ä½œæœ¬èº«æ˜¯åŒæ­¥çš„ï¼Œä½†æ§½å‡½æ•°çš„è§¦å‘æ˜¯å¼‚æ­¥çš„ã€‚
                        #
                        # éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå¦‚æžœæ‚¨åœ¨ä¸»çº¿ç¨‹ä¸­è°ƒç”¨ emit()ï¼Œé‚£ä¹ˆæ§½å‡½æ•°å°†ä¼šåœ¨ä¸»çº¿ç¨‹ä¸­è¢«å¼‚æ­¥åœ°è°ƒç”¨ï¼›
                        # å¦‚æžœæ‚¨åœ¨éžä¸»çº¿ç¨‹ä¸­è°ƒç”¨ emit()ï¼Œé‚£ä¹ˆæ§½å‡½æ•°å°†ä¼šåœ¨ä¸Žè¯¥å­çº¿ç¨‹ç›¸å¯¹åº”çš„ä¸»çº¿ç¨‹ä¸­è¢«å¼‚æ­¥åœ°è°ƒç”¨ï¼Œ
                        # è¿™æ˜¯ç”±äºŽ PySide6 æˆ– PyQt6 çš„çº¿ç¨‹æ¨¡åž‹æ‰€å†³å®šçš„ã€‚
                        self.yolo2main_res_img.emit(im0)  # after detection  ----------ç»“æžœ
                        self.yolo2main_pre_img.emit(im0s if isinstance(im0s, np.ndarray) else im0s[0])  # Before testing
                        # self.yolo2main_labels.emit(self.labels_dict)        # webcam need to change the def write_results
                        self.yolo2main_class_num.emit(class_nums)
                        self.yolo2main_target_num.emit(target_nums)

                        print('send success!')

                        if self.speed_thres != 0:
                            time.sleep(self.speed_thres / 1000)  # delay , ms

                    self.yolo2main_progress.emit(self.progress_value)  # progress bar

                # Detection completed
                if count + 1 >= all_count:
                    if isinstance(self.vid_writer[-1], cv2.VideoWriter):
                        self.vid_writer[-1].release()  # release final video writer
                    self.yolo2main_status_msg.emit('Detection completed')
                    break

        except Exception as e:
            print(e)
            self.yolo2main_status_msg.emit('%s' % e)

    def get_annotator(self, img):
        return Annotator(img, line_width=self.args.line_thickness, example=str(self.model.names))

    def preprocess(self, img):
        img = torch.from_numpy(img).to(self.model.device)
        img = img.half() if self.model.fp16 else img.float()  # uint8 to fp16/32
        img /= 255  # 0 - 255 to 0.0 - 1.0
        return img

    def postprocess(self, preds, img, orig_img):
        ### important
        preds = ops.non_max_suppression(preds,
                                        self.conf_thres,
                                        self.iou_thres,
                                        agnostic=self.args.agnostic_nms,
                                        max_det=self.args.max_det,
                                        classes=self.args.classes)

        results = []
        for i, pred in enumerate(preds):
            orig_img = orig_img[i] if isinstance(orig_img, list) else orig_img
            shape = orig_img.shape
            pred[:, :4] = ops.scale_boxes(img.shape[2:], pred[:, :4], shape).round()
            path, _, _, _, _ = self.batch
            img_path = path[i] if isinstance(path, list) else path
            results.append(Results(orig_img=orig_img, path=img_path, names=self.model.names, boxes=pred))
        # print(results)
        return results

    def write_results(self, idx, results, batch):
        p, im, im0 = batch
        log_string = ''
        if len(im.shape) == 3:
            im = im[None]  # expand for batch dim
        self.seen += 1
        imc = im0.copy() if self.args.save_crop else im0
        if self.source_type.webcam or self.source_type.from_img:  # batch_size >= 1         # attention
            log_string += f'{idx}: '
            frame = self.dataset.count
        else:
            frame = getattr(self.dataset, 'frame', 0)
        self.data_path = p
        self.txt_path = str(self.save_dir / 'labels' / p.stem) + ('' if self.dataset.mode == 'image' else f'_{frame}')
        # log_string += '%gx%g ' % im.shape[2:]         # !!! don't add img size~
        self.annotator = self.get_annotator(im0)

        det = results[idx].boxes  # TODO: make boxes inherit from tensors

        if len(det) == 0:
            return f'{log_string}(no detections), '  # if no, send this~~

        for c in det.cls.unique():
            n = (det.cls == c).sum()  # detections per class
            log_string += f"{n}~{self.model.names[int(c)]},"  # {'s' * (n > 1)}, "   # don't add 's'
        # now log_string is the classes ðŸ‘†

        # write
        for d in reversed(det):
            cls, conf = d.cls.squeeze(), d.conf.squeeze()
            if self.save_txt:  # Write to file
                line = (cls, *(d.xywhn.view(-1).tolist()), conf) \
                    if self.args.save_conf else (cls, *(d.xywhn.view(-1).tolist()))  # label format
                with open(f'{self.txt_path}.txt', 'a') as f:
                    f.write(('%g ' * len(line)).rstrip() % line + '\n')
            if self.save_res or self.args.save_crop or self.args.show or True:  # Add bbox to image(must)
                c = int(cls)  # integer class
                name = f'id:{int(d.id.item())} {self.model.names[c]}' if d.id is not None else self.model.names[c]
                label = None if self.args.hide_labels else (name if self.args.hide_conf else f'{name} {conf:.2f}')
                self.annotator.box_label(d.xyxy.squeeze(), label, color=colors(c, True))
            if self.args.save_crop:
                save_one_box(d.xyxy,
                             imc,
                             file=self.save_dir / 'crops' / self.model.model.names[c] / f'{self.data_path.stem}.jpg',
                             BGR=True)

        return log_string
